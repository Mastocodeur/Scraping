{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6f333bd-2609-4858-b410-1414c00cd3d6",
   "metadata": {},
   "source": [
    "# <center>Scraping Activity</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e655cd2-053b-47a2-baf0-9eabfc01b7dc",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/scrappeur.png\" width=\"600\" height=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e036870a-d66a-4179-a37c-cbc845d4af33",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f4c6a6-7867-4f2f-8653-df8ceea4569c",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aeb85f9-b3c0-4541-a7cf-9085482f8cdb",
   "metadata": {},
   "source": [
    "Let's start by setting out the basics of scraping. Scraping means knowing how to read what's behind a site. By simply right-clicking and inspecting the element, you can access the page's HTML code quite easily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a46eec-b1f5-46c9-837f-956dcef224c4",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/du site au html.png\" width=\"800\" height=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408ba027-bc50-4306-ab47-58cd2b1ca486",
   "metadata": {},
   "source": [
    "Here's a little  [HTML Form](form_html.md). It's a quick way of learning or remembering the main tags used to read HTML and identify the different elements of a web page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c58b036-74aa-43bd-8739-cbde358365a1",
   "metadata": {},
   "source": [
    "Next, choose a scraping tool: Selenium or BeautifulSoup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd0c361-3575-4c36-978e-3bddfb9ce167",
   "metadata": {},
   "source": [
    "- Selenium is useful for dynamic web pages where content is generated via JavaScript, requiring user interaction such as clicking, scrolling or text input.\n",
    "\n",
    "- BeautifulSoup is a Python library used primarily for parsing HTML and XML documents. It is useful for extracting structured data from static web pages.\n",
    "\n",
    "Generally speaking, we prefer to use Selenium, which allows more actions, but beautifulsoup remains an important option. In this activity, we present the two modules to help you get to grips with them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a673eb-db4d-48ac-a680-2c1a2fc47a80",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b651b072-df16-4586-8b2b-6ea80422c1da",
   "metadata": {},
   "source": [
    "## Activity 1a : Recover a Professor Layton riddle with Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abe749d-4ac0-44b2-a5f4-a2a1d61f7b69",
   "metadata": {},
   "source": [
    "We're going to see how we can simply retrieve the main elements of a wiki page to create a database. To do this, we're going to connect to https://professeur-layton.fandom.com/fr/wiki/La_travers%C3%A9e_(1). \n",
    "\n",
    "The idea will initially be to recover :\n",
    "- the title\n",
    "- the riddle number\n",
    "- the statement,\n",
    "- the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f454db-be86-4540-b80b-8f78634070c6",
   "metadata": {},
   "source": [
    "###### Importing python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50be1ee0-400d-4ae5-ad6d-4fa10515ed55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee0ba2e-8543-4dc9-8b6c-5fb43f87c517",
   "metadata": {},
   "source": [
    "###### Launch the browser and connect to the riddle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809ce0e9-4b2d-4224-a7a5-ab3260a25755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mise en place du driver chrome\n",
    "service = Service(executable_path=ChromeDriverManager().install())\n",
    "# Ajouter des options \n",
    "chrome_options = Options()\n",
    "# Désactiver la propriété qui révèle le contrôle par l'automatisation\n",
    "chrome_options.add_argument('--disable-blink-features=AutomationControlled')  \n",
    "# On lance Chrome en fournissant le driver et en renseignant les options\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "# On renseigne l'URL que l'on veut scraper\n",
    "url = \"https://professeur-layton.fandom.com/fr/wiki/La_travers%C3%A9e_(1)\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd46d9c-4ce6-46d5-8a46-c1e06552e73f",
   "metadata": {},
   "source": [
    "###### Clever recovery of Web elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07ff8bf-f906-477d-aebe-12710764cbc4",
   "metadata": {},
   "source": [
    "This is where we introduce the various Web element selectors : \n",
    "\n",
    "- ID = \"id\"\n",
    "- NAME = \"name\"\n",
    "- XPATH = \"xpath\"\n",
    "- LINK_TEXT = \"link text\"\n",
    "- PARTIAL_LINK_TEXT = \"partial link text\"\n",
    "- TAG_NAME = \"tag name\"\n",
    "- CLASS_NAME = \"class name\"\n",
    "- CSS_SELECTOR = \"css selector\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107d68a7-75c6-47da-9444-d9ed397a3b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title recovery :\n",
    "title = driver.find_element(By.XPATH ,'//*[@id=\"firstHeading\"]')\n",
    "\n",
    "# Find the riddle number :\n",
    "num_enigme = driver.find_element(By.XPATH ,'//*[@id=\"mw-content-text\"]/div/div[1]/div[2]/table/tbody/tr[4]/td')\n",
    "\n",
    "# Retrieving the statement :\n",
    "enonce = driver.find_element(By.XPATH ,'//*[@id=\"Énoncé\"]')\n",
    "enigme_enonce = enonce.find_elements(By.XPATH, '//h2[span[@id=\"Énoncé\"]]/following-sibling::p | //h2[span[@id=\"Énoncé\"]]/following-sibling::ul')\n",
    "\n",
    "# Retrieving the answer :\n",
    "reponse = driver.find_element(By.XPATH, '//*[@id=\"mw-content-text\"]/div/p[8]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d45a493-7cc7-42f3-8929-83e1b98b4159",
   "metadata": {},
   "source": [
    "###### Reading Web elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6947042-4355-42f5-8b1f-0693207b949a",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = title.text\n",
    "num_enigme = num_enigme.text\n",
    "enigme_enonce = [elem.text for elem in enigme_enonce]\n",
    "enigme_enonce = \"\\n\".join(enigme_enonce)\n",
    "reponse = reponse.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d377e730-ebbf-4aae-8c74-ccee3087e128",
   "metadata": {},
   "source": [
    "###### Storage in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2ff1f7-af2c-4d77-b396-9c4ff1291b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the list dictionary\n",
    "data = {\n",
    "    'title': title,\n",
    "    'number': num_enigme,\n",
    "    'description': enigme_enonce,\n",
    "    'solution': reponse\n",
    "}\n",
    "# Print data\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7387a7-921e-4ba5-b75e-b5d6ce70a10c",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7048a99-f382-4a77-8903-1d0894fa5e58",
   "metadata": {},
   "source": [
    "## Activity 1b : Recover a Professor Layton riddle with BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e864852-a2ae-4ade-b39c-5b22ce681b9d",
   "metadata": {},
   "source": [
    "###### Importing python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf63d1cd-4ba9-41be-81b0-e796cfca220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des bibliothèques\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a05b38-e998-4ee4-90ae-3c148945110f",
   "metadata": {},
   "source": [
    "###### Launch a request to retrieve all the source code of the url to be scrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89165e0d-05be-4788-83be-4aa7a1b89f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://professeur-layton.fandom.com/fr/wiki/La_travers%C3%A9e_(1)\"\n",
    "data  = requests.get(url).text\n",
    "soup = BeautifulSoup(data,\"html5lib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c615aae-9c55-4615-99b5-47f661218d68",
   "metadata": {},
   "source": [
    "###### Display page source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c488c02-9e52-4041-8c9d-e92b67c18081",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ead3f57-2b0f-4d39-b084-098eece9aa66",
   "metadata": {},
   "source": [
    "###### Recovering Web elements from soup.prettify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f23b4d-60c9-4f64-9f53-93ad5ad96860",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = soup.find('meta', attrs={'property': \"og:title\"})\n",
    "title = title.get(\"content\")\n",
    "print(title)\n",
    "print(\"\\n\")\n",
    "\n",
    "url_enigme = soup.find('meta', attrs={'property': \"og:url\"})\n",
    "print(url_enigme.get(\"content\"))\n",
    "\n",
    "# Trouver la balise <tr> contenant \"Professeur Layton et l'Étrange Village\"\n",
    "layton_row = soup.find('a', title=\"Professeur Layton et l'Étrange Village\").find_parent('tr')\n",
    "# Trouver la balise <tr> suivante\n",
    "numero_row = layton_row.find_next_sibling('tr')\n",
    "# Extraire le contenu de la balise <td> contenant le numéro\n",
    "numero = numero_row.find('td').text.strip()\n",
    "print(numero)\n",
    "\n",
    "print(\"\\n\")\n",
    "start_element = soup.find('span', id='Énoncé').find_parent('h2')\n",
    "\n",
    "# Initialize an empty list to collect the text\n",
    "text_list = []\n",
    "\n",
    "# Iterate over all next siblings of the start element\n",
    "for sibling in start_element.find_next_siblings():\n",
    "    if sibling.name == 'h2':  # Stop if we reach another h2 element\n",
    "        break\n",
    "    if sibling.name in ['p', 'ul']:  # Collect text from p and ul elements\n",
    "        text_list.append(sibling.get_text())\n",
    "\n",
    "# Join the collected text into a single string\n",
    "enigme_enonce = \"\\n\".join(text_list)\n",
    "\n",
    "# Print the resulting string\n",
    "print(enigme_enonce)\n",
    "\n",
    "##########\n",
    "\n",
    "start_element = soup.find('span', id='Résolution').find_parent('h3')\n",
    "\n",
    "# Initialize an empty list to collect the text\n",
    "text_list = []\n",
    "\n",
    "# Iterate over all next siblings of the start element\n",
    "for sibling in start_element.find_next_siblings():\n",
    "    if sibling.name == 'h3':  # Stop if we reach another h2 element\n",
    "        break\n",
    "    if sibling.name in ['p', 'ul']:  # Collect text from p and ul elements\n",
    "        text_list.append(sibling.get_text())\n",
    "\n",
    "# Join the collected text into a single string\n",
    "reponse = \" \".join(text_list)\n",
    "print(reponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0866fea-81dd-49f5-9369-b530eb56475c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the list dictionary\n",
    "data = {\n",
    "    'title': title,\n",
    "    'number': numero,\n",
    "    'descripton': enigme_enonce,\n",
    "    'solution': reponse\n",
    "}\n",
    "\n",
    "# Print data\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4dd990-fa58-4cf6-9872-dff74dee60f9",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92163375-fd98-47d7-9f1f-02ef3ce61d00",
   "metadata": {},
   "source": [
    "## Activity 2 : Your turn !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4fbb5a-ac98-410c-a183-1516cc518677",
   "metadata": {},
   "source": [
    "In this second part, we suggest you do the same thing for several puzzles. To do this, we'll give you the part of the code that retrieves all the riddle urls and we'll select 5 at random to avoid saturating this little wiki."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3a73f1-7f1d-4772-8bb0-a77624b5b1e6",
   "metadata": {},
   "source": [
    "###### Launch a request to retrieve all the source code of the url to be scrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dda5af0-d083-4063-8275-6c17301cb57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://professeur-layton.fandom.com/fr/wiki/Cat%C3%A9gorie:%C3%89nigmes\"\n",
    "data  = requests.get(url).text\n",
    "soup = BeautifulSoup(data,\"html5lib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ecca56-e498-4c9b-8749-eb9f39025349",
   "metadata": {},
   "source": [
    "###### Display page source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e714ce-2a3d-4761-a7c4-979050258a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1ea655-d96a-4e0d-882a-7d789e1d39ec",
   "metadata": {},
   "source": [
    "###### Collect all the puzzle links and take just 5 at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d68197-ddca-462b-8ec3-e866b94fdd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all <a> elements with the ‘category-page__member-link’ class.\n",
    "links = soup.find_all('a', class_='category-page__member-link')\n",
    "\n",
    "# Extract href attributes\n",
    "hrefs = [link.get('href') for link in links]\n",
    "\n",
    "# Display hrefs\n",
    "#print(hrefs)\n",
    "#print(len(hrefs))\n",
    "\n",
    "# Randomly select 5 hrefs\n",
    "hrefs = random.sample(hrefs, 5)\n",
    "\n",
    "# Display randomly selected hrefs\n",
    "#print(random_hrefs)\n",
    "#print(len(random_hrefs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64fd639-3e47-48a7-895c-8c4357dd072d",
   "metadata": {},
   "source": [
    "###### Let's create a function to repeat the data extraction steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5195ef-d717-40bf-ab40-8881deec1831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collecte_enigme(racine, href):\n",
    "    url = racine+href\n",
    "    data  = requests.get(url).text\n",
    "    soup = BeautifulSoup(data,\"html.parser\")\n",
    "  \n",
    "    # Title recovery\n",
    "    title = ...\n",
    "    title = ...\n",
    "\n",
    "    # Recovering urls\n",
    "    url_enigme = ...\n",
    "    url_enigme = ...\n",
    "\n",
    "    # Retrieving the riddle\n",
    "    enigme_title = ...\n",
    "    # Find the paragraph following the title of the ‘Statement’ section\n",
    "    enigme_paragraph = ...\n",
    "    # Extract text from paragraph\n",
    "    enigme = enigme_paragraph.get_text(strip=True)\n",
    "        \n",
    "    \n",
    "        \n",
    "    # Retrieving the answer\n",
    "    reponse_title = ...\n",
    "    reponse_paragraph = ...\n",
    "    reponse = reponse_paragraph.get_text(strip=True)\n",
    "  \n",
    "    # Append the collected data as a dictionary\n",
    "    data = []\n",
    "    data.append({'Title': title, 'url': url_enigme, 'Enigme': enigme, 'Solution': reponse})\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2a839e-1db8-4d74-9a8b-802bf0892707",
   "metadata": {},
   "source": [
    "###### Using the function and storing it in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c795a828-861d-4234-a5ee-27631ac10395",
   "metadata": {},
   "outputs": [],
   "source": [
    "racine = \"https://professeur-layton.fandom.com\"\n",
    "df_final = pd.DataFrame()\n",
    "for href in hrefs:\n",
    "    #print(href)\n",
    "    df_final = pd.concat([df_final, collecte_enigme(racine,href)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145124c0-949a-435a-b620-98c23523a31a",
   "metadata": {},
   "source": [
    "###### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f36477-174a-4fa7-9ac7-938272a1cb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_row', 7)\n",
    "pd.set_option('display.max_column', 6)\n",
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2e582c-54d1-4617-82ae-d05b7ec943bf",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137e12c8-22b2-407b-94df-ae5e7601e28d",
   "metadata": {},
   "source": [
    "We finally managed to build a DataFrame from a website! The most important thing to remember is that BeautifulSoup is very useful for ‘open’ sites and for the massive repetition of data collection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
